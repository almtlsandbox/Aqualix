#!/usr/bin/env python3
"""
Script de test global pour Aqualix
ExÃ©cute tous les tests du systÃ¨me auto-tune
"""

import sys
import os
import subprocess
from pathlib import Path

def run_test(test_path, test_name):
    """
    ExÃ©cute un test spÃ©cifique
    
    Args:
        test_path: Chemin vers le fichier de test
        test_name: Nom descriptif du test
    """
    print(f"\nğŸ§ª EXÃ‰CUTION: {test_name}")
    print("=" * 50)
    
    try:
        # ExÃ©cuter depuis le rÃ©pertoire racine du projet
        root_dir = Path(__file__).parent.parent
        result = subprocess.run([sys.executable, test_path], 
                               capture_output=True, text=True, cwd=root_dir)
        
        if result.returncode == 0:
            print(f"âœ… {test_name}: RÃ‰USSI")
            return True
        else:
            print(f"âŒ {test_name}: Ã‰CHEC")
            if result.stderr:
                print(f"Erreur: {result.stderr[:200]}...")
            return False
            
    except Exception as e:
        print(f"âŒ {test_name}: ERREUR D'EXÃ‰CUTION - {e}")
        return False

def main():
    """ExÃ©cute tous les tests du systÃ¨me auto-tune"""
    
    print("ğŸš€ SUITE DE TESTS GLOBALE AQUALIX AUTO-TUNE")
    print("=" * 60)
    
    tests_root = Path(__file__).parent
    
    # Tests auto-tune (nos nouveaux tests)
    autotune_tests = [
        (tests_root / "autotune" / "test_autotune_mapping.py", "Auto-Tune Mapping System (Ã‰tape 3)"),
        (tests_root / "autotune" / "test_quality_metrics.py", "Quality Metrics Integration (Ã‰tape 4)"),
    ]
    
    # Tests d'intÃ©gration
    integration_tests = [
        (tests_root / "integration" / "test_quality_check.py", "Quality Check Integration"),
    ]
    
    # Tests UI (si ils fonctionnent)
    ui_tests = [
        (tests_root / "ui" / "test_dialog_import.py", "Dialog Import Test"),
    ]
    
    results = []
    
    # ExÃ©cution des tests auto-tune
    print("\nğŸ“‚ TESTS AUTO-TUNE")
    print("-" * 30)
    for test_path, test_name in autotune_tests:
        if test_path.exists():
            success = run_test(test_path, test_name)
            results.append((test_name, success))
        else:
            print(f"âš ï¸  {test_name}: Fichier non trouvÃ© - {test_path}")
            results.append((test_name, False))
    
    # ExÃ©cution des tests d'intÃ©gration
    print("\nğŸ“‚ TESTS INTÃ‰GRATION")
    print("-" * 30)
    for test_path, test_name in integration_tests:
        if test_path.exists():
            success = run_test(test_path, test_name)
            results.append((test_name, success))
        else:
            print(f"âš ï¸  {test_name}: Fichier non trouvÃ© - {test_path}")
            results.append((test_name, False))
    
    # ExÃ©cution des tests UI
    print("\nğŸ“‚ TESTS UI")
    print("-" * 30)
    for test_path, test_name in ui_tests:
        if test_path.exists():
            success = run_test(test_path, test_name)
            results.append((test_name, success))
        else:
            print(f"âš ï¸  {test_name}: Fichier non trouvÃ© - {test_path}")
            results.append((test_name, False))
    
    # RÃ©sumÃ© final
    print("\nğŸ¯ RÃ‰SUMÃ‰ GLOBAL")
    print("=" * 60)
    
    total_tests = len(results)
    passed_tests = sum(1 for _, success in results if success)
    failed_tests = total_tests - passed_tests
    
    print(f"ğŸ“Š Tests exÃ©cutÃ©s: {total_tests}")
    print(f"âœ… Tests rÃ©ussis: {passed_tests}")
    print(f"âŒ Tests Ã©chouÃ©s: {failed_tests}")
    print(f"ğŸ“ˆ Taux de rÃ©ussite: {(passed_tests/total_tests*100):.1f}%")
    
    print("\nğŸ“‹ DÃ‰TAILS:")
    for test_name, success in results:
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {test_name}")
    
    if passed_tests == total_tests:
        print(f"\nğŸ‰ TOUS LES TESTS RÃ‰USSIS! SystÃ¨me auto-tune opÃ©rationnel.")
        return 0
    else:
        print(f"\nâš ï¸  {failed_tests} test(s) en Ã©chec. VÃ©rification nÃ©cessaire.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
